# Crawling Service - State of Play
# Updated: 2026-02-07 (post-div4k1 code audit)

## Summary

Universal access crawling system for trucking carrier competitive intelligence.

**Status**: Div 4k1 implementation is largely landed. Validation, tuning, and coverage expansion remain.

Completed:
- Orchestrator refactored (`crawl.py`: 2162 -> ~523 lines)
- Capture pipeline is default execution path
- Recon -> method selection wired (SPA detection works)
- `orchestrate/` modules extracted (`config`, `fetch_spec`, `presenter`)
- Div 4k1 closed-loop components implemented:
  - `fetch/access_classifier.py` (outcome classification)
  - `fetch/access_policy.py` (escalation policy/backoff)
  - adaptive bounded retry loop in `scripts/crawl.py`
  - attempt/outcome telemetry persisted into site JSON
- Educational docs created (`docs/edu/`)

Remaining:
- Full tier crawls to improve corpus coverage
- Live defended-domain validation/tuning for escalation thresholds
- Access telemetry/report alignment cleanup
- SLO measurement after corpus is populated across tiers

**Reality check**: Access layer is no longer one-shot/open-loop by default, but most Tier-2/Tier-3 domains still lack crawl data.


## What Works Right Now

```bash
# Crawl tier-1 carriers (adaptive escalation enabled by default)
python scripts/crawl.py --tier 1 --limit 5

# Force static (single-attempt-like) behavior if needed
python scripts/crawl.py --tier 1 --limit 5 --access-escalation-mode static

# Check access status
python scripts/access_report.py

# Run focused div4k1 tests
pytest -q tests/test_access_classifier.py tests/test_access_policy.py

# Process blocked sites manually
python scripts/monkey.py --next
```


## Architecture (Current)

```
scripts/crawl.py (~523 lines)
    |
    |- CLI parsing
    |- orchestrate.config -> load seeds, profiles, run config
    |- orchestrate.fetch_spec -> resolve fetch method + access hints
    |- capture_site() -> adaptive attempt loop + telemetry
    \- orchestrate.presenter -> build site JSON

orchestrate/
|- config.py        # Seeds, profiles, freshness checking
|- fetch_spec.py    # Method resolution, FetchConfig/access hint building
\- presenter.py     # Site JSON construction + access summary

fetch/
|- capture.py             # Two-phase capture pipeline
|- extractor.py           # Content extraction + block tagging
|- recon.py               # CDN/WAF fingerprinting
|- access_classifier.py   # Outcome classification
|- access_policy.py       # Strategy escalation policy
|- monkey.py              # Record/replay system
\- human.py               # Human emulation (mouse, timing)
```


## Div Series Status

| Div | Purpose | Status |
|-----|---------|--------|
| 1 | Core crawl pipeline | COMPLETE |
| 2 | Compliance & structured data | COMPLETE |
| 3 | Comp packages monitoring | PARTIAL (scripts exist, not fully wired) |
| 4 | Adaptive access layer | IMPLEMENTED, tuning/validation in progress |
| 4a-4h | Access subsystems | COMPLETE |
| 4i | Capture/extract refactor | COMPLETE |
| 4j | Parallelization, Docker | COMPLETE |
| 4k | Orchestrator refactor + QA | ARCHITECTURE DONE, QA/HARDENING REMAIN |
| 4k1 | Closed-loop access control | CORE IMPLEMENTATION LANDED |
| 5 | Operator integration | PLANNING ONLY |

### What "Core Implementation Landed" Means

Implemented:
- [x] Access outcome model (`AccessOutcome`, `AccessAttempt`)
- [x] Classifier and policy modules
- [x] Bounded adaptive retry loop in `crawl.py`
- [x] Terminal-failure monkey auto-enqueue path
- [x] Access telemetry in site outputs (`access_telemetry`, `access_summary`)
- [x] New unit tests for classifier/policy

Still needed:
- [ ] Full live crawl validation (especially defended domains)
- [ ] Tighten report/telemetry consistency for escalation metrics
- [ ] Recompute SLOs after broader crawl coverage


## Current Metrics

From local run of `python scripts/access_report.py` on 2026-02-07:

| Metric | Current | Target |
|--------|---------|--------|
| Overall success | 18.4% | 80%+ |
| Tier-1 success | 75.0% | 95% |
| Block rate | 4.1% | <10% |
| HTTP efficiency | 56.5% | 60% |

Root cause of low overall success: missing crawl data for most Tier-2/Tier-3 domains, not purely hard blocks.


## Known Issues

1. **Telemetry consistency gap**: per-URL `final_outcome` in `access_telemetry` is recorded before post-extraction reclassification, so summaries can diverge from final page-level outcomes.

2. **Execution log warning (historical)**: intermittent `'total_word_count'` warning was reported; needs verification under current code path.

3. **Coverage debt**: many domains still `no_crawl_data`, limiting metric interpretability.


## Quick Reference

```bash
# Crawl single domain
python scripts/crawl.py --domain jbhunt.com --depth 2

# Tier crawl with parallel workers
python scripts/crawl.py --tier 1 -j 4 --progress

# Cap adaptive retries
python scripts/crawl.py --tier 1 --access-max-attempts 2

# Docker crawl (virtual display)
./scripts/docker_crawl.sh --tier 1 --limit 5

# Check blocked sites / outcomes
python scripts/access_report.py

# Process monkey queue
python scripts/monkey.py --list
python scripts/monkey.py --next

# Focused div4k1 tests
pytest -q tests/test_access_classifier.py tests/test_access_policy.py
```


## Key Files

```
scripts/
|- crawl.py                # Main entry (adaptive access loop wired)
|- access_report.py        # SLO + outcome metrics
|- monkey.py               # Human-in-loop CLI
|- eval_extraction.py      # Auto-evaluate extractions
|- render_extraction.py    # HTML reports
\- export.py               # Data export

fetch/
|- access_classifier.py    # Access outcome classification
|- access_policy.py        # Escalation logic + plan building
\- capture_config.py       # AccessOutcome/AccessAttempt models

orchestrate/
|- config.py               # load_seeds, load_run_config, freshness
|- fetch_spec.py           # resolve_fetch_spec, access hints
\- presenter.py            # site JSON + access summary helpers

docs/
|- div4k1.md               # Closed-loop access design/rollout doc
|- div4k.txt               # Orchestrator/QA plan
\- stateofplay.txt         # This file
```


## Next Steps

1. Run controlled live validations on a small defended-domain set and tune classifier/policy thresholds.
2. Run broader tier crawls to reduce `no_crawl_data` and produce stable SLO baselines.
3. Reconcile `access_report.py` with new telemetry fields end-to-end.
4. Rebaseline targets/roadmap after first post-div4k1 full crawl pass.

The core adaptive loop is now in place. The bottleneck is quality tuning plus crawl coverage at scale.
